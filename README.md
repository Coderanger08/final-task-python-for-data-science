## Project Overview: Automated Job Role Classification from Resumes

This project addresses the challenge of automatically classifying job roles from an unlabelled dataset of 155 resumes, focusing purely on the skills listed in each resume.

### Initial Approach: Rule-Based Classification

The first step involved creating a rule-based classifier to generate baseline `rule_label`s. This system identified job roles (e.g., SOC Analyst, .NET Developer, Data Engineer) by matching specific keywords within the `skills` section of each resume. If no rules were matched, the resume was categorized as 'Other'.

### LLM-Based Classification: Initial Attempts

1.  **`flan-t5-small` with Prompt Engineering:** An initial attempt utilized the `google/flan-t5-small` model with a carefully crafted prompt. The model was instructed to act as an expert technical recruiter and classify job roles based on skills. However, this approach resulted in a highly skewed distribution of predictions, with a significant majority of resumes being classified as 'SOC Analyst', indicating a lack of effective distinction between diverse skill sets.

2.  **Zero-Shot Classification with `facebook/bart-large-mnli`:** To establish a performance baseline, a zero-shot classification approach was employed using `facebook/bart-large-mnli`. This model, a RoBERTa-like encoder, was used to predict job roles without any prior training on our specific dataset. The performance metrics were:
    *   Accuracy: 0.0968
    *   Precision (weighted): 0.5717
    *   Recall (weighted): 0.0968
    *   F1-score (weighted): 0.1110

    These results demonstrated that the generalized knowledge of the BART model was insufficient for this domain-specific task, highlighting the need for a more targeted approach.

### Breakthrough: Fine-Tuning `roberta-base`

Given the limitations of generalized and zero-shot models, the project pivoted to fine-tuning a `roberta-base` model. The `rule_label` dataset, generated by the initial rule-based system, served as the ground truth for training. The process involved:

1.  **Data Preparation:** Mapping string labels to numerical IDs and tokenizing the `skills` data using the `roberta-base` tokenizer.
2.  **Dataset Creation:** Creating a custom PyTorch `Dataset` for efficient batch processing.
3.  **Train-Test Split:** Dividing the dataset into 80% for training and 20% for evaluation.
4.  **Model Fine-tuning:** Training the `roberta-base` model for sequence classification using `TrainingArguments` and a `Trainer` from the Hugging Face Transformers library.

### Results of Fine-Tuning:

Fine-tuning led to a dramatic improvement in performance:

*   **Accuracy:** Increased from ~9.7% (zero-shot) to **0.6774** (~67.7%).
*   **F1-score (weighted):** Increased from 0.1110 (zero-shot) to **0.5471**.

### Conclusion

The project successfully demonstrated the critical impact of fine-tuning for domain-specific tasks. By training a `roberta-base` model on our `rule_label` dataset, we achieved a substantial improvement in job role classification accuracy and F1-score, effectively transforming raw skill data into reliably classified job roles. This highlights that while LLMs offer broad capabilities, targeted adaptation through fine-tuning is often essential for achieving high performance on niche tasks.
